# Technology Trends Analysis
## A project to analyze technology trends using data obtained from APIs, Web Scraping and Survey data
![](IMAGES/toptrendsjpg.jpg)
## Introduction
In order to keep pace with changing technologies and remain competitive, organizations regularly analyze data to help identify current technologies as well as future skill requirements.
This project aim is therefore to carry out technology trend analysis which will start with collecting data from various sources and  clean it up to make it suitable for the analysis.

##  Problem statement 
1. __What are the skill requirements for future?__
2. __What are the top programming languages in demand?__
3. __What are the top database skills in demand?__
 
 
## Skills demonstrated.
- I used python for web scrapping and data retrival from api
- I used jupyter notebooks and vscode as IDE for the project
- I used IBM Cognos dashboard tool for visualization.

## Data sourcing
We scrape data of  the **name of the programming language** and **average annual salary** from the <a href="https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/labs/datasets/Programming_Languages.html">URL</a>  provided by skills network labs. 
We then write the scraped data into an csv file

We then use the <a href="https://developer.adzuna.com/">Adzuna API</a> to scrape data of number of technology jobs in some selected US cities and store them in an excel file 

We also use the Stack Overflow Developer Survey 2019 data to create a dashboard with IBM Cognos visualization tool to visualize and present our analysis<br>
Note:This randomised subset contains around 1/10th of the original data set.<br> Our Conclusions drawn after analyzing this subset may not reflect the real world scenario<br>
To access this dataset from IBM cloud <a href="https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/LargeData/m1_survey_data.csv">Click_HERE</a>

## Data transformation and cleaning.
1.  The survey dataset goes through various cleaning processes before being visualized in the IBM Cognos tool.
2.  Data from both the Api and scraped web page also undergoes cleaning and reformating before being visualized.

## Data analysis and visualization
Graph showing number of jobs generated by each technology![](IMAGES/tech_jobs.png)
### key take ways from the graph above
- Java technology had the most number of job opportunities among all the languages
- The next three languages that followed according to job opportunities created were GO, SQL and Python

Graph showing salaries for different technologies![](IMAGES/salary_language.png)
## Cluster building
To perform clustering, we had to preprocess the data using principle component analysis algorithm to produce two principle componets as our dimensions to be used in the clustering process.These were used to generate the clusters as seen in the image above. Further anlysis was performed to find the ideal number of clusters to be used in our data.We considered the ideal number of clusters to be used as 3 that can be seen in the graph above.

Graph showing cluster analysis with data![](IMAGES/clusterstat.jpg)
Having performed some statistics on the generated clusters, we can draw the following conclusions
1. Cluster 1 has the highest average income of about 13k and average age of 46 years.This can be considerd as a high value target cluster for banking products such as loans and savings.
2. Cluster 2 has the least average income of about 9.4k and has the least average age of 29 years. This can be a target cluster for bank products such as mortgage and credit card
3. Cluster 0 has a moderate average income of about 9.7k and has the higest average age of 84 years. This can be a target cluster for bank products such as pension


## Final model buiding and evaluation
Model performance graph![](IMAGES/modal_image1.png)

A range of models were trained on the data and their performance assessed as seen in the bar graph above.
The gradient boosting classifier and the Random forest model were the best performing classifers.
Further analysis was performed to know which of them was the better model.The Random forest emerged as the slightly better model by comparing the performance of thier classification reports and confusion matrices. 
The best performing model was saved for use in the streamlit application.

## Model deployment
Streamlit Application user interface![](IMAGES/streamlitapp.jpg)

## Conclussion

1. The clusters generated give us some useful information about our customer base such as their average income,
   age,debt to income ratio, education and years employed. 
2. We can use the statistics about these clusters to know where each customer belongs to.
3. With this information, we can then be able to tailor our bank products such as savings, loans, mortgage
   credit card to specific customers depending on the cluster they belong to for better performance of marketing     campaigns.

